{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to get All caps count from the tweets\n",
    "# pass tweets as it is\n",
    "def FuncAllCapsCount(GivenTweets):\n",
    "    AllCapsCount=np.zeros(len(GivenTweets))\n",
    "    for i in tqdm(range(len(GivenTweets))):\n",
    "        tweet=nltk.word_tokenize(GivenTweets[i])\n",
    "        for word in tweet:\n",
    "            if(word!=\"I\" and re.match(\"^[A-Z]+$\",word)):\n",
    "                AllCapsCount[i]+=1\n",
    "    return(AllCapsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to get HashTag count from the tweets\n",
    "# pass tweets as it is\n",
    "def FuncHashtagsCount(GivenTweets):    \n",
    "    HashTagsCount=np.zeros(len(GivenTweets))\n",
    "    for i in tqdm(range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i].split(\" \")\n",
    "        for word in tweet:\n",
    "            if(re.match(\"^#[a-z]\",word)):\n",
    "                HashTagsCount[i]+=1\n",
    "    return(HashTagsCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Punctuations like ??? ?!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucntion to get COntinuous Punctuation (???,??!,!!) Count and check if last token was a ? or !\n",
    "# pass tweets as it is\n",
    "def FuncPunctuationsCount(GivenTweets):\n",
    "    ContPunctuationCount=np.zeros(len(GivenTweets))\n",
    "    LastTokenPunctuation=np.zeros(len(GivenTweets))\n",
    "    for i in tqdm(range(len(GivenTweets))):\n",
    "        tweet=nltk.word_tokenize(GivenTweets[i])\n",
    "        token=0\n",
    "        while token<len(tweet):\n",
    "            if(tweet[token]==\"?\" or tweet[token]==\"!\"):\n",
    "                index=token+1\n",
    "                while(index< len(tweet) and (tweet[index]==\"?\" or tweet[index]==\"!\")):\n",
    "                    index+=1\n",
    "                if(index-token>1):\n",
    "                    ContPunctuationCount[i]+=1\n",
    "                    token=index\n",
    "            token+=1\n",
    "        if(tweet[len(tweet)-1]==\"?\" or tweet[len(tweet)-1]==\"!\"):\n",
    "            LastTokenPunctuation[i]+=1\n",
    "    return(ContPunctuationCount,LastTokenPunctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the above functions to extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('sentiment_train.csv')\n",
    "data_test = pd.read_csv('sentiment_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting All caps count and saving it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [03:49<00:00, 4878.49it/s]\n",
      "100%|██████████| 480000/480000 [01:35<00:00, 5049.09it/s]\n"
     ]
    }
   ],
   "source": [
    "Train_AllCapsCount=FuncAllCapsCount(data_train['5'])\n",
    "Test_AllCapsCount=FuncAllCapsCount(data_test['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_AllCapsCount=pd.DataFrame(data=Train_AllCapsCount).astype('int')\n",
    "df_Test_AllCapsCount=pd.DataFrame(data=Test_AllCapsCount).astype('int')\n",
    "df_Train_AllCapsCount.to_csv(\"df_Train_AllCapsCount.csv\",index=False)\n",
    "df_Test_AllCapsCount.to_csv(\"df_Test_AllCapsCount.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Hashtag count and saving it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [00:15<00:00, 70996.88it/s]\n",
      "100%|██████████| 480000/480000 [00:07<00:00, 68499.16it/s]\n"
     ]
    }
   ],
   "source": [
    "Train_FuncHashtagsCount=FuncHashtagsCount(data_train['5'])\n",
    "Test_FuncHashtagsCount=FuncHashtagsCount(data_test['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_FuncHashtagsCount=pd.DataFrame(data=Train_FuncHashtagsCount).astype('int')\n",
    "df_Test_FuncHashtagsCount=pd.DataFrame(data=Test_FuncHashtagsCount).astype('int')\n",
    "df_Train_FuncHashtagsCount.to_csv(\"df_Train_FuncHashtagsCount.csv\",index=False)\n",
    "df_Test_FuncHashtagsCount.to_csv(\"df_Test_FuncHashtagsCount.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Continuous Punctuations and last token ?,! presence and saving it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [03:39<00:00, 5093.32it/s]\n",
      "100%|██████████| 480000/480000 [01:30<00:00, 5304.63it/s]\n"
     ]
    }
   ],
   "source": [
    "Train_ContPunctuationCount,Train_LastTokenPunctuation=FuncPunctuationsCount(data_train['5'])\n",
    "Test_ContPunctuationCount,Test_LastTokenPunctuation=FuncPunctuationsCount(data_test['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_ContPunctuationCount=pd.DataFrame(data=Train_ContPunctuationCount).astype('int')\n",
    "df_Test_ContPunctuationCount=pd.DataFrame(data=Test_ContPunctuationCount).astype('int')\n",
    "df_Train_ContPunctuationCount.to_csv(\"df_Train_ContPunctuationCount.csv\",index=False)\n",
    "df_Test_ContPunctuationCount.to_csv(\"df_Test_ContPunctuationCount.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_LastTokenPunctuation=pd.DataFrame(data=Train_LastTokenPunctuation).astype('int')\n",
    "df_Test_LastTokenPunctuation=pd.DataFrame(data=Test_LastTokenPunctuation).astype('int')\n",
    "df_Train_LastTokenPunctuation.to_csv(\"df_Train_LastTokenPunctuation.csv\",index=False)\n",
    "df_Test_LastTokenPunctuation.to_csv(\"df_Test_LastTokenPunctuation.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to construct a set of all the possible POS tags for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the set of all the possible tags from train and test\n",
    "# Also calculating the count of each tags in train and test set\n",
    "def PosDictBuild(TrainTweets,TestTweets):\n",
    "    mytags=set()\n",
    "    TrainTags=[]\n",
    "    TestTags=[]\n",
    "    TrainTweets=TrainTweets.apply(lambda s:s.lower())\n",
    "    TestTweets=TestTweets.apply(lambda s:s.lower())\n",
    "    for i in tqdm(range(len(TrainTweets))):\n",
    "        TempDic={}\n",
    "        tweet=nltk.word_tokenize(TrainTweets[i])\n",
    "        tags=nltk.pos_tag(tweet)\n",
    "        for i in tags:\n",
    "            mytags.add(i[1])\n",
    "            if(i[1] in TempDic):\n",
    "                TempDic[i[1]]+=1\n",
    "            else:\n",
    "                TempDic[i[1]]=1\n",
    "        TrainTags.append(TempDic)\n",
    "    for i in tqdm(range(len(TestTweets))):\n",
    "        TempDic={}\n",
    "        tweet=nltk.word_tokenize(TestTweets[i])\n",
    "        tags=nltk.pos_tag(tweet)\n",
    "        for i in tags:\n",
    "            mytags.add(i[1])\n",
    "            if(i[1] in TempDic):\n",
    "                TempDic[i[1]]+=1\n",
    "            else:\n",
    "                TempDic[i[1]]=1\n",
    "        TestTags.append(TempDic)\n",
    "    return(mytags,TrainTags,TestTags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 480000/480000 [07:47<00:00, 1026.14it/s]\n"
     ]
    }
   ],
   "source": [
    "mytags,TrainTags,TestTags=PosDictBuild(data_train['5'],data_test['5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file1=open(\"TrainTags\",'wb')\n",
    "pickle.dump(TrainTags,file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file2=open(\"TestTags\",'wb')\n",
    "pickle.dump(TestTags,file2)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainTagsArray = np.zeros((len(TrainTags), len(mytags)), int)\n",
    "TestTagsArray = np.zeros((len(TestTags), len(mytags)), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping of tags and integers for the feature vector\n",
    "counter=0\n",
    "PosTagDictionary={}\n",
    "RevPosTagDictionary={}\n",
    "for i in mytags:\n",
    "    PosTagDictionary[counter]=i\n",
    "    RevPosTagDictionary[i]=counter\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the POS tag feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [00:05<00:00, 202048.36it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(TrainTags))):\n",
    "    for j in TrainTags[i]:\n",
    "        TrainTagsArray[i][RevPosTagDictionary[j]]=TrainTags[i][j]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 480000/480000 [00:02<00:00, 207441.47it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(TestTags))):\n",
    "    for j in TestTags[i]:\n",
    "        TestTagsArray[i][RevPosTagDictionary[j]]=TestTags[i][j]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_Pos_Tags=pd.DataFrame(data=TrainTagsArray)\n",
    "df_Train_Pos_Tags.rename(columns=PosTagDictionary,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Test_Pos_Tags=pd.DataFrame(data=TestTagsArray)\n",
    "df_Test_Pos_Tags.rename(columns=PosTagDictionary,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_Pos_Tags.to_csv(\"df_Train_Pos_Tags.csv\",index=False)\n",
    "df_Test_Pos_Tags.to_csv(\"df_Test_Pos_Tags.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions for Lexicon Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fucnction to construct the BingLiu and MPQA feature and the NRC word emotion association lexicon\n",
    "# It includes the count of positive and negative words for a particular tweet\n",
    "# Give tweets after preprocessing\n",
    "def FuncBingLiu_MPQA_NRCemotion(GivenTweets):\n",
    "    mpqa_pos=set()\n",
    "    mpqa_neg=set()\n",
    "    bing_pos=set()\n",
    "    bing_neg=set()\n",
    "    bing_data = pd.read_csv(\"./lexicons/1. BingLiu.csv\", sep='\\t', names=[\"words\",\"sentiment\"], header=None)\n",
    "\n",
    "    for i in range(len(bing_data['words'])):\n",
    "        if bing_data['sentiment'][i]=='positive':\n",
    "            bing_pos.add(bing_data['words'][i])\n",
    "        elif bing_data['sentiment'][i]=='negative':\n",
    "            bing_neg.add(bing_data['words'][i])\n",
    "    mpqa_data = pd.read_csv(\"./lexicons/2. mpqa.txt\", sep='\\t', names=[\"words\",\"sentiment\"], header=None)\n",
    "\n",
    "    for i in range(len(mpqa_data['words'])):\n",
    "        if mpqa_data['sentiment'][i]=='positive':\n",
    "            mpqa_pos.add(mpqa_data['words'][i])\n",
    "        elif mpqa_data['sentiment'][i]=='negative':\n",
    "            mpqa_neg.add(mpqa_data['words'][i])\n",
    "\n",
    "    polar_pos=mpqa_pos.union(bing_pos)\n",
    "    polar_neg=mpqa_neg.union(bing_neg)\n",
    "\n",
    "    emotions_df=pd.read_csv(\"./lexicons/8. NRC-word-emotion-lexicon.txt\", sep='\\t', names=['word','emotion','score'], header=None)\n",
    "    emotions_df.head()\n",
    "    \n",
    "    for i in range(len(emotions_df['word'])):\n",
    "        if emotions_df['emotion'][i]==\"positive\" and emotions_df['score'][i]==1:\n",
    "            polar_pos.add(emotions_df['word'][i])\n",
    "\n",
    "        elif emotions_df['emotion'][i]==\"negative\" and emotions_df['score'][i]==1:\n",
    "            polar_neg.add(emotions_df['word'][i])\n",
    "\n",
    "\n",
    "    polarcount_pos=np.zeros(len(GivenTweets))\n",
    "    polarcount_neg=np.zeros(len(GivenTweets))\n",
    "    for i in tqdm(range(len(GivenTweets))):\n",
    "        tweet=GivenTweets[i]\n",
    "        for word in word_tokenize(tweet):\n",
    "            if word in polar_pos:\n",
    "                polarcount_pos[i]+=1\n",
    "            elif word in polar_neg:\n",
    "                polarcount_neg[i]+=1\n",
    "    return(polarcount_pos,polarcount_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Hashtag Sentiment Lexicon\n",
    "# Pass the tweets as it is\n",
    "def Func_NRCHashtagSentiment(Giventweets):\n",
    "    d1 = pd.read_csv(\"./lexicons/7. NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\", sep='\\t', names=['term','score','numPos','numNeg'], header=None)\n",
    "    dic_d1={}\n",
    "    for i in range(len(d1['term'])):\n",
    "        dic_d1[d1['term'][i]]=d1['score'][i]\n",
    "    # print(dic_d1)\n",
    "    aggScoreHashtags=np.zeros(len(Giventweets))\n",
    "    for i in tqdm(range(len(Giventweets))):\n",
    "        tweet=Giventweets[i]\n",
    "        hashtags = [i  for i in tweet.split() if i.startswith(\"#\") ]\n",
    "        for word in hashtags:\n",
    "            if word in dic_d1:\n",
    "                aggScoreHashtags[i]+=dic_d1[word]\n",
    "            elif word[1:] in dic_d1:\n",
    "                aggScoreHashtags[i]+=dic_d1[word[1:]]\n",
    "    return(aggScoreHashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for sentiment 140 lexicon\n",
    "# Give tweets after pre processing\n",
    "def Func_Sentiment140Lexicon(Giventweets):\n",
    "    d1 = pd.read_csv(\"./lexicons/3. Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\", sep='\\t', names=['term','score','numPos','numNeg'], header=None)\n",
    "    dic_d1={}\n",
    "    for i in tqdm(range(len(d1['term']))):\n",
    "        dic_d1[d1['term'][i]]=d1['score'][i]\n",
    "    aggScore1=np.zeros(len(Giventweets))\n",
    "    for i in tqdm(range(len(Giventweets))):\n",
    "        tweet=Giventweets[i]\n",
    "        for word in word_tokenize(tweet):\n",
    "            if word in dic_d1:\n",
    "                aggScore1[i]+=dic_d1[word]\n",
    "    return(aggScore1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to count the elongated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the Number of elongated words for each tweet\n",
    "# (.)\\1 represents the repetition of continuous characters\n",
    "# {2,} the repetition should be more than 2\n",
    "def elongated_words(Giventweets):\n",
    "    ElongatedWordCount=np.zeros(len(Giventweets))\n",
    "    for i in tqdm(range(len(Giventweets))):\n",
    "        tweet=Giventweets[i]\n",
    "        for word in word_tokenize(tweet):\n",
    "            if(len(re.findall(r\"(.)\\1{2,}\",word))>0):\n",
    "                ElongatedWordCount[i]+=1\n",
    "    return(ElongatedWordCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to count the emoticon score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticon_score(Giventweets):\n",
    "    emoticons=pd.read_csv(\"./lexicons/9. AFINN-emoticon-8.txt\", sep='\\t', names=['emoticon','score'], header=None)\n",
    "    dic_emoji={}\n",
    "    for i in tqdm(range(len(emoticons['emoticon']))):\n",
    "        dic_emoji[emoticons['emoticon'][i]]=emoticons['score'][i]\n",
    "    PresencePositiveEmoticon=np.zeros(len(Giventweets))\n",
    "    PresenceNegativeEmoticon=np.zeros(len(Giventweets))\n",
    "    LastTokenEmoticon=np.zeros(len(Giventweets))\n",
    "    emoji = re.compile('[\\\\u203C-\\\\u3299\\\\U0001F000-\\\\U0001F644]')\n",
    "    emojiScore=np.zeros(len(Giventweets))\n",
    "    for i in tqdm(range(len(Giventweets))):\n",
    "        tweet=(Giventweets[i].split(\" \"))\n",
    "        for i in range(len(tweet)):\n",
    "            if(tweet[i] in dic_emoji):\n",
    "                if(dic_emoji[tweet[i]]>0):\n",
    "                    PresencePositiveEmoticon[i]+=dic_emoji[tweet[i]]\n",
    "                elif(dic_emoji[tweet[i]]<0):\n",
    "                    PresenceNegativeEmoticon[i]+=dic_emoji[tweet[i]]\n",
    "        if(tweet[len(tweet)-1] in dic_emoji ):\n",
    "            if(dic_emoji[tweet[len(tweet)-1]]>0):\n",
    "                LastTokenEmoticon[i]=1\n",
    "            elif(dic_emoji[tweet[len(tweet)-1]]<0):\n",
    "                LastTokenEmoticon[i]=0\n",
    "    return(PresencePositiveEmoticon,PresenceNegativeEmoticon,LastTokenEmoticon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to count the nagated contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass tweets as it is and remove punctuations here\n",
    "def negation_count(GivenTweets):\n",
    "    # Count of Negating words\n",
    "    neg_context_count=np.zeros(len(GivenTweets))\n",
    "    for i in tqdm(range(len(GivenTweets))):\n",
    "        tweet=nltk.sent_tokenize(GivenTweets[i])\n",
    "        for j in tweet:\n",
    "            context=word_tokenize(j)\n",
    "            context[0]=re.sub(r'[^\\w\\s]',\"\",context[0])\n",
    "            res = re.match(r'never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint',context[0])\n",
    "            if(res and re.match(r\",|.|:|;|!|\\?\",context[len(context)-1])):\n",
    "#                 print(context[0],context[len(context)-1])\n",
    "                neg_context_count[i]+=1\n",
    "    return(neg_context_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using these functions to extract the features and saving them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('sentiment_train.csv')\n",
    "data_test = pd.read_csv('sentiment_test.csv')\n",
    "tweets_train_original=data_train['5']\n",
    "tweets_train=data_train['5']\n",
    "tweets_test_original=data_test['5']\n",
    "tweets_test=data_test['5']\n",
    "\n",
    "tweets_train=tweets_train.apply(lambda s:s.lower())\n",
    "tweets_train_original=tweets_train_original.apply(lambda s:s.lower())\n",
    "\n",
    "tweets_test=tweets_test.apply(lambda s:s.lower())\n",
    "tweets_test_original=tweets_test_original.apply(lambda s:s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [01:08<00:00, 16251.18it/s]\n",
      "100%|██████████| 480000/480000 [00:24<00:00, 19932.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(tweets_train))):\n",
    "    #Removing Urls, mentions, hashtags\n",
    "    res=p.clean(tweets_train[i])\n",
    "    res = re.sub(r'[^\\w\\s]', '',res)\n",
    "    tweets_train[i]=res\n",
    "for i in tqdm(range(len(tweets_test))):\n",
    "    #Removing Urls, mentions, hashtags\n",
    "    res=p.clean(tweets_test[i])\n",
    "    res = re.sub(r'[^\\w\\s]', '',res)\n",
    "    tweets_test[i]=res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [02:21<00:00, 7930.91it/s]\n",
      "100%|██████████| 480000/480000 [00:54<00:00, 8771.04it/s]\n"
     ]
    }
   ],
   "source": [
    "train_polarcount_pos,train_polarcount_neg=FuncBingLiu_MPQA_NRCemotion(tweets_train)\n",
    "test_polarcount_pos,test_polarcount_neg=FuncBingLiu_MPQA_NRCemotion(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_polarcount=pd.DataFrame(train_polarcount_pos)\n",
    "df_train_polarcount.rename(columns={0:\"polarcount_positive\"},inplace=True)\n",
    "df_train_polarcount[\"polarcount_negative\"]=pd.DataFrame(train_polarcount_neg)\n",
    "df_train_polarcount[\"polarcount_negative\"]=df_train_polarcount[\"polarcount_negative\"].astype(int)\n",
    "df_train_polarcount[\"polarcount_positive\"]=df_train_polarcount[\"polarcount_positive\"].astype(int)\n",
    "df_train_polarcount.to_csv(\"df_train_polarcount.csv\",index=False)\n",
    "\n",
    "\n",
    "df_test_polarcount=pd.DataFrame(test_polarcount_pos)\n",
    "df_test_polarcount.rename(columns={0:\"polarcount_positive\"},inplace=True)\n",
    "df_test_polarcount[\"polarcount_negative\"]=pd.DataFrame(test_polarcount_neg)\n",
    "df_test_polarcount[\"polarcount_negative\"]=df_test_polarcount[\"polarcount_negative\"].astype(int)\n",
    "df_test_polarcount[\"polarcount_positive\"]=df_test_polarcount[\"polarcount_positive\"].astype(int)\n",
    "df_test_polarcount.to_csv(\"df_test_polarcount.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43431/43431 [00:00<00:00, 112206.05it/s]\n",
      "100%|██████████| 1120000/1120000 [02:10<00:00, 8590.94it/s]\n",
      "100%|██████████| 43431/43431 [00:00<00:00, 114248.85it/s]\n",
      "100%|██████████| 480000/480000 [00:53<00:00, 8986.02it/s]\n"
     ]
    }
   ],
   "source": [
    "train_aggScore=Func_Sentiment140Lexicon(tweets_train)\n",
    "test_aggScore1=Func_Sentiment140Lexicon(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_aggScore=pd.DataFrame(train_aggScore)\n",
    "df_train_aggScore.rename(columns={0:\"Sentiment140Score\"},inplace=True)\n",
    "df_train_aggScore.to_csv(\"df_train_aggScore.csv\",index=False)\n",
    "\n",
    "df_test_aggScore=pd.DataFrame(test_aggScore1)\n",
    "df_test_aggScore.rename(columns={0:\"Sentiment140Score\"},inplace=True)\n",
    "df_test_aggScore.to_csv(\"df_test_aggScore.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [00:05<00:00, 192187.59it/s]\n",
      "100%|██████████| 480000/480000 [00:02<00:00, 210393.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_HashtagScore=Func_NRCHashtagSentiment(tweets_train_original)\n",
    "test_HashtagScore=Func_NRCHashtagSentiment(tweets_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_HashtagScore=pd.DataFrame(train_HashtagScore)\n",
    "df_train_HashtagScore.rename(columns={0:\"NRChashTagscore\"},inplace=True)\n",
    "df_train_HashtagScore.to_csv(\"df_train_HashtagScore.csv\",index=False)\n",
    "\n",
    "df_test_HashtagScore=pd.DataFrame(test_HashtagScore)\n",
    "df_test_HashtagScore.rename(columns={0:\"NRChashTagscore\"},inplace=True)\n",
    "df_test_HashtagScore.to_csv(\"df_test_HashtagScore.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [03:33<00:00, 5247.94it/s]\n",
      "100%|██████████| 480000/480000 [01:28<00:00, 5395.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ElongatedWordCount=elongated_words(tweets_train_original)\n",
    "test_ElongatedWordCount=elongated_words(tweets_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ElongatedWordCount=pd.DataFrame(train_ElongatedWordCount)\n",
    "df_train_ElongatedWordCount.rename(columns={0:\"ELongatedWordCOunt\"},inplace=True)\n",
    "df_train_ElongatedWordCount[\"ELongatedWordCOunt\"]=df_train_ElongatedWordCount[\"ELongatedWordCOunt\"].astype(int)\n",
    "df_train_ElongatedWordCount.to_csv(\"df_train_ElongatedWordCount.csv\",index=False)\n",
    "\n",
    "df_test_ElongatedWordCount=pd.DataFrame(test_ElongatedWordCount)\n",
    "df_test_ElongatedWordCount.rename(columns={0:\"ELongatedWordCOunt\"},inplace=True)\n",
    "df_test_ElongatedWordCount[\"ELongatedWordCOunt\"]=df_test_ElongatedWordCount[\"ELongatedWordCOunt\"].astype(int)\n",
    "df_test_ElongatedWordCount.to_csv(\"df_test_ElongatedWordCount.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 27032.78it/s]\n",
      "100%|██████████| 1120000/1120000 [00:06<00:00, 186603.60it/s]\n",
      "100%|██████████| 96/96 [00:00<00:00, 103085.81it/s]\n",
      "100%|██████████| 480000/480000 [00:02<00:00, 193021.12it/s]\n"
     ]
    }
   ],
   "source": [
    "Train_PositiveEmoticonScore,Train_NegativeEmoticonScore,Train_LastTokenEmoticon=emoticon_score(tweets_train_original)\n",
    "Test_PositiveEmoticonScore,Test_NegativeEmoticonScore,Test_LastTokenEmoticon=emoticon_score(tweets_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_Emoticon=pd.DataFrame(Train_PositiveEmoticonScore)\n",
    "df_train_Emoticon.rename(columns={0:\"PositiveEmoticonScore\"},inplace=True)\n",
    "df_train_Emoticon[\"NegativeEmoticonScore\"]=pd.DataFrame(Train_NegativeEmoticonScore)\n",
    "df_train_Emoticon[\"LastTokenEmoticon\"]=pd.DataFrame(Train_LastTokenEmoticon)\n",
    "\n",
    "df_train_Emoticon[\"PositiveEmoticonScore\"]=df_train_Emoticon[\"PositiveEmoticonScore\"].astype(int)\n",
    "df_train_Emoticon[\"NegativeEmoticonScore\"]=df_train_Emoticon[\"NegativeEmoticonScore\"].astype(int)\n",
    "df_train_Emoticon[\"LastTokenEmoticon\"]=df_train_Emoticon[\"LastTokenEmoticon\"].astype(int)\n",
    "\n",
    "df_train_Emoticon.to_csv(\"df_train_Emoticon.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "df_test_Emoticon=pd.DataFrame(Test_PositiveEmoticonScore)\n",
    "df_test_Emoticon.rename(columns={0:\"PositiveEmoticonScore\"},inplace=True)\n",
    "df_test_Emoticon[\"NegativeEmoticonScore\"]=pd.DataFrame(Test_NegativeEmoticonScore)\n",
    "df_test_Emoticon[\"LastTokenEmoticon\"]=pd.DataFrame(Test_LastTokenEmoticon)\n",
    "\n",
    "df_test_Emoticon[\"PositiveEmoticonScore\"]=df_test_Emoticon[\"PositiveEmoticonScore\"].astype(int)\n",
    "df_test_Emoticon[\"NegativeEmoticonScore\"]=df_test_Emoticon[\"NegativeEmoticonScore\"].astype(int)\n",
    "df_test_Emoticon[\"LastTokenEmoticon\"]=df_test_Emoticon[\"LastTokenEmoticon\"].astype(int)\n",
    "\n",
    "# df_test_Emoticon.head()\n",
    "df_test_Emoticon.to_csv(\"df_test_Emoticon.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test_Emoticon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1120000/1120000 [04:17<00:00, 4353.58it/s]\n",
      "100%|██████████| 480000/480000 [01:49<00:00, 4391.92it/s]\n"
     ]
    }
   ],
   "source": [
    "Train_negation_context_count=negation_count(tweets_train_original)\n",
    "Test_negation_context_count=negation_count(tweets_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Train_negation_context_count=pd.DataFrame(Train_negation_context_count)\n",
    "df_Train_negation_context_count.rename(columns={0:\"negation_context_count\"},inplace=True)\n",
    "df_Train_negation_context_count[\"negation_context_count\"]=df_Train_negation_context_count[\"negation_context_count\"].astype(int)\n",
    "df_Train_negation_context_count.to_csv(\"df_Train_negation_context_count.csv\",index=False)\n",
    "\n",
    "df_Test_negation_context_count=pd.DataFrame(Test_negation_context_count)\n",
    "df_Test_negation_context_count.rename(columns={0:\"negation_context_count\"},inplace=True)\n",
    "df_Test_negation_context_count[\"negation_context_count\"]=df_Test_negation_context_count[\"negation_context_count\"].astype(int)\n",
    "df_Test_negation_context_count.to_csv(\"df_Test_negation_context_count.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating all features except ngrams ans saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Feature=pd.read_csv(\"./Question1 Features/df_Train_AllCapsCount.csv\")\n",
    "Train_Feature.rename(columns={'0':\"AllCapsCount\"},inplace=True)\n",
    "Train_Feature['ContPunctuationCount']=pd.read_csv(\"./Question1 Features/df_Train_ContPunctuationCount.csv\")\n",
    "Train_Feature['ELongatedWordCount']=pd.read_csv(\"./Question1 Features/df_train_ElongatedWordCount.csv\")\n",
    "Train_Feature[[\"PositiveEmoticonScore\",\"NegativeEmoticonScore\",\"LastTokenEmoticon\"]]=pd.read_csv(\"./Question1 Features/df_train_Emoticon.csv\")\n",
    "Train_Feature['HashTagCount']=pd.read_csv(\"./Question1 Features/df_Train_FuncHashtagsCount.csv\")\n",
    "Train_Feature['NRChashTagscore']=pd.read_csv(\"./Question1 Features/df_train_HashtagScore.csv\")\n",
    "Train_Feature['LastTokenPunctuation']=pd.read_csv(\"./Question1 Features/df_Train_LastTokenPunctuation.csv\")\n",
    "Train_Feature['NegationContextCount']=pd.read_csv(\"./Question1 Features/df_Train_negation_context_count.csv\")\n",
    "Train_Feature[['polarcount_positive', 'polarcount_negative']]=pd.read_csv(\"./Question1 Features/df_train_polarcount.csv\")\n",
    "Train_Feature[['RP', 'JJ', '\\'\\'', 'EX', 'VBG', ')', 'NN', 'MD', 'PRP$', 'JJS', 'WP$','IN', 'JJR', ':', 'PDT', 'DT', '(', 'NNPS', ',', 'RBR', '``', 'PRP','WDT', 'VBP', 'VBN', 'SYM', 'WRB', 'RBS', 'VBZ', 'UH', 'CD', 'VB', '#','TO', '$', 'RB', 'NNS', 'FW', 'WP', 'CC', 'POS', 'VBD', '.', 'LS','NNP']]=pd.read_csv(\"./Question1 Features/df_Train_Pos_Tags.csv\")\n",
    "Train_Feature[\"Sentiment140Score\"]=pd.read_csv(\"./Question1 Features/df_train_aggScore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Feature.to_csv(\"TrainFeatures_without_ngrams\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Feature=pd.read_csv(\"./Question1 Features/df_Test_AllCapsCount.csv\")\n",
    "Test_Feature.rename(columns={'0':\"AllCapsCount\"},inplace=True)\n",
    "Test_Feature['ContPunctuationCount']=pd.read_csv(\"./Question1 Features/df_Test_ContPunctuationCount.csv\")\n",
    "Test_Feature['ELongatedWordCount']=pd.read_csv(\"./Question1 Features/df_test_ElongatedWordCount.csv\")\n",
    "Test_Feature[[\"PositiveEmoticonScore\",\"NegativeEmoticonScore\",\"LastTokenEmoticon\"]]=pd.read_csv(\"./Question1 Features/df_test_Emoticon.csv\")\n",
    "Test_Feature['HashTagCount']=pd.read_csv(\"./Question1 Features/df_Test_FuncHashtagsCount.csv\")\n",
    "Test_Feature['NRChashTagscore']=pd.read_csv(\"./Question1 Features/df_test_HashtagScore.csv\")\n",
    "Test_Feature['LastTokenPunctuation']=pd.read_csv(\"./Question1 Features/df_Test_LastTokenPunctuation.csv\")\n",
    "Test_Feature['NegationContextCount']=pd.read_csv(\"./Question1 Features/df_Test_negation_context_count.csv\")\n",
    "Test_Feature[['polarcount_positive', 'polarcount_negative']]=pd.read_csv(\"./Question1 Features/df_test_polarcount.csv\")\n",
    "Test_Feature[['RP', 'JJ', '\\'\\'', 'EX', 'VBG', ')', 'NN', 'MD', 'PRP$', 'JJS', 'WP$','IN', 'JJR', ':', 'PDT', 'DT', '(', 'NNPS', ',', 'RBR', '``', 'PRP','WDT', 'VBP', 'VBN', 'SYM', 'WRB', 'RBS', 'VBZ', 'UH', 'CD', 'VB', '#','TO', '$', 'RB', 'NNS', 'FW', 'WP', 'CC', 'POS', 'VBD', '.', 'LS','NNP']]=pd.read_csv(\"./Question1 Features/df_Test_Pos_Tags.csv\")\n",
    "Test_Feature[\"Sentiment140Score\"]=pd.read_csv(\"./Question1 Features/df_test_aggScore.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Feature.to_csv(\"TestFeatures_without_ngrams\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('../input/task1a3/sentiment_train.csv')\n",
    "data_test = pd.read_csv('../input/task1a3/sentiment_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TrainFeature_without_ngram=pd.read_csv(\"../input/nlp-a3-ques1-sklearn-features/TrainFeatures_without_ngrams\")\n",
    "df_TestFeature_without_ngram=pd.read_csv(\"../input/nlp-a3-ques1-sklearn-features/TestFeatures_without_ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train=data_train['5']\n",
    "tweets_test=data_test['5']\n",
    "\n",
    "tweets_train=tweets_train.apply(lambda s:s.lower())\n",
    "tweets_test=tweets_test.apply(lambda s:s.lower())\n",
    "\n",
    "TrainRange=50000\n",
    "TestRange=len(tweets_test)\n",
    "\n",
    "tweets_train=tweets_train[0:TrainRange]\n",
    "tweets_test=tweets_test[0:TestRange]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alltweets=tweets_train\n",
    "Alltweets=Alltweets.append(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# ngram_range parameter (1,2) means that unigram and bigram will be taken\n",
    "# Count Vecotrizer automatically preprocess the tweets\n",
    "vectorizer = CountVectorizer(ngram_range=(1,4),min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit was used to learn the vocabulary\n",
    "TempVector=vectorizer.fit(Alltweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the vocabulary for ngrams\n",
    "Vocab_ngrams=TempVector.get_feature_names()\n",
    "len(Vocab_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocab={}\n",
    "for i in range(len(Vocab_ngrams)):\n",
    "    Vocab[i]=Vocab_ngrams[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"Dictionay_ngram_features_50k\",'wb')\n",
    "pickle.dump(Vocab,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use this vocabulary to individually create ngrams for train and test set\n",
    "# fit transform is used here as we want the counts too\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1,4), vocabulary=Vocab_ngrams)\n",
    "Train_ngrams=vectorizer2.fit_transform(tweets_train)\n",
    "Test_ngrams=vectorizer2.fit_transform(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"Train_ngrams_50k\",'wb')\n",
    "pickle.dump(Train_ngrams,file)\n",
    "file.close()\n",
    "file=open(\"Test_ngrams_50ktrain\",'wb')\n",
    "pickle.dump(Test_ngrams,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "csr_TrainFeature_without_ngram=csr_matrix(df_TrainFeature_without_ngram[0:TrainRange])\n",
    "csr_TestFeature_without_ngram=csr_matrix(df_TestFeature_without_ngram[0:TestRange])\n",
    "TrainFeatureFinal=hstack([Train_ngrams,csr_TrainFeature_without_ngram])\n",
    "TestFeatureFinal=hstack([Test_ngrams,csr_TestFeature_without_ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTmodel=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTmodel.fit(TrainFeatureFinal,data_train['0'][0:TrainRange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"DTmodel_50k\",'wb')\n",
    "pickle.dump(DTmodel,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480000,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2=DTmodel.predict(TestFeatureFinal)\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"y_DT_50k\",'wb')\n",
    "pickle.dump(y2,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(verbose=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(verbose=True)\n",
    "clf.fit(TrainFeatureFinal,data_train['0'][0:TrainRange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open(\"SVCmodel50ktrain\",'wb')\n",
    "pickle.dump(clf,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ySCV=clf.predict(TestFeatureFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file1=open(\"ySCV50ktrain\",'wb')\n",
    "pickle.dump(ySCV,file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47223788\n",
      "Iteration 2, loss = 0.39944030\n",
      "Iteration 3, loss = 0.35827391\n",
      "Iteration 4, loss = 0.32101355\n",
      "Iteration 5, loss = 0.28645006\n",
      "Iteration 6, loss = 0.25407663\n",
      "Iteration 7, loss = 0.21971458\n",
      "Iteration 8, loss = 0.18896948\n",
      "Iteration 9, loss = 0.15951808\n",
      "Iteration 10, loss = 0.13196118\n",
      "Iteration 11, loss = 0.11430425\n",
      "Iteration 12, loss = 0.09454833\n",
      "Iteration 13, loss = 0.07680971\n",
      "Iteration 14, loss = 0.06594767\n",
      "Iteration 15, loss = 0.05872366\n",
      "Iteration 16, loss = 0.04925504\n",
      "Iteration 17, loss = 0.04767966\n",
      "Iteration 18, loss = 0.03809580\n",
      "Iteration 19, loss = 0.03311983\n",
      "Iteration 20, loss = 0.03060653\n",
      "Iteration 21, loss = 0.02958855\n",
      "Iteration 22, loss = 0.02779880\n",
      "Iteration 23, loss = 0.02760848\n",
      "Iteration 24, loss = 0.02220098\n",
      "Iteration 25, loss = 0.02152374\n",
      "Iteration 26, loss = 0.02021733\n",
      "Iteration 27, loss = 0.02132274\n",
      "Iteration 28, loss = 0.01823085\n",
      "Iteration 29, loss = 0.01740286\n",
      "Iteration 30, loss = 0.01525037\n",
      "Iteration 31, loss = 0.01518245\n",
      "Iteration 32, loss = 0.01704904\n",
      "Iteration 33, loss = 0.01659506\n",
      "Iteration 34, loss = 0.01423757\n",
      "Iteration 35, loss = 0.01342346\n",
      "Iteration 36, loss = 0.01345468\n",
      "Iteration 37, loss = 0.01677518\n",
      "Iteration 38, loss = 0.01325692\n",
      "Iteration 39, loss = 0.01497176\n",
      "Iteration 40, loss = 0.01191729\n",
      "Iteration 41, loss = 0.01092763\n",
      "Iteration 42, loss = 0.01360101\n",
      "Iteration 43, loss = 0.01361557\n",
      "Iteration 44, loss = 0.01101072\n",
      "Iteration 45, loss = 0.01024761\n",
      "Iteration 46, loss = 0.01004568\n",
      "Iteration 47, loss = 0.01035062\n",
      "Iteration 48, loss = 0.01138944\n",
      "Iteration 49, loss = 0.00984874\n",
      "Iteration 50, loss = 0.00950186\n",
      "Iteration 51, loss = 0.01024691\n",
      "Iteration 52, loss = 0.01024150\n",
      "Iteration 53, loss = 0.01047323\n",
      "Iteration 54, loss = 0.00969554\n",
      "Iteration 55, loss = 0.00936100\n",
      "Iteration 56, loss = 0.01173886\n",
      "Iteration 57, loss = 0.00895633\n",
      "Iteration 58, loss = 0.00921765\n",
      "Iteration 59, loss = 0.01003269\n",
      "Iteration 60, loss = 0.00883342\n",
      "Iteration 61, loss = 0.00844172\n",
      "Iteration 62, loss = 0.00839330\n",
      "Iteration 63, loss = 0.01004503\n",
      "Iteration 64, loss = 0.00948818\n",
      "Iteration 65, loss = 0.01086944\n",
      "Iteration 66, loss = 0.01013462\n",
      "Iteration 67, loss = 0.00855538\n",
      "Iteration 68, loss = 0.00806014\n",
      "Iteration 69, loss = 0.00791496\n",
      "Iteration 70, loss = 0.00775759\n",
      "Iteration 71, loss = 0.00797373\n",
      "Iteration 72, loss = 0.00815053\n",
      "Iteration 73, loss = 0.00790712\n",
      "Iteration 74, loss = 0.00806914\n",
      "Iteration 75, loss = 0.00811082\n",
      "Iteration 76, loss = 0.01010476\n",
      "Iteration 77, loss = 0.00815929\n",
      "Iteration 78, loss = 0.00777701\n",
      "Iteration 79, loss = 0.00761757\n",
      "Iteration 80, loss = 0.00740332\n",
      "Iteration 81, loss = 0.00747656\n",
      "Iteration 82, loss = 0.00703789\n",
      "Iteration 83, loss = 0.00792935\n",
      "Iteration 84, loss = 0.00726572\n",
      "Iteration 85, loss = 0.00715682\n",
      "Iteration 86, loss = 0.00698971\n",
      "Iteration 87, loss = 0.00731311\n",
      "Iteration 88, loss = 0.00721720\n",
      "Iteration 89, loss = 0.00726978\n",
      "Iteration 90, loss = 0.01028875\n",
      "Iteration 91, loss = 0.00954372\n",
      "Iteration 92, loss = 0.00757228\n",
      "Iteration 93, loss = 0.00682515\n",
      "Iteration 94, loss = 0.00657876\n",
      "Iteration 95, loss = 0.00738745\n",
      "Iteration 96, loss = 0.00712432\n",
      "Iteration 97, loss = 0.00686490\n",
      "Iteration 98, loss = 0.00645205\n",
      "Iteration 99, loss = 0.00693133\n",
      "Iteration 100, loss = 0.00682827\n",
      "Iteration 101, loss = 0.00668464\n",
      "Iteration 102, loss = 0.00687185\n",
      "Iteration 103, loss = 0.00707188\n",
      "Iteration 104, loss = 0.00684994\n",
      "Iteration 105, loss = 0.00642679\n",
      "Iteration 106, loss = 0.00715183\n",
      "Iteration 107, loss = 0.00846852\n",
      "Iteration 108, loss = 0.00736342\n",
      "Iteration 109, loss = 0.00782642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=300, verbose=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf2 = MLPClassifier(max_iter=300,verbose=True)\n",
    "clf2.fit(TrainFeatureFinal,data_train['0'][0:TrainRange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file2=open(\"MLPmodel_50ktrain\",'wb')\n",
    "pickle.dump(clf2,file2)\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y4=clf2.predict(TestFeatureFinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file3=open(\"yMLP50ktrain\",'wb')\n",
    "pickle.dump(y4,file3)\n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file3=open(\"TrainFeatureFinal\",'wb')\n",
    "pickle.dump(TrainFeatureFinal,file3)\n",
    "file3.close()\n",
    "file3=open(\"TestFeatureFinal\",'wb')\n",
    "pickle.dump(TestFeatureFinal,file3)\n",
    "file3.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocessor as p\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.svm import SVC\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==0):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==0):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==4):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Predicted Negative','Predicted Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Actual Negative\", 1: \"Actual Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('sentiment_train.csv')\n",
    "data_test = pd.read_csv('sentiment_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"TrainFeatureFinal\",\"rb\")\n",
    "Train=pickle.load(file)\n",
    "file1=open(\"TestFeatureFinal\",\"rb\")\n",
    "Test=pickle.load(file1)\n",
    "file.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 54414.54it/s]\n"
     ]
    }
   ],
   "source": [
    "Username={}\n",
    "GivenTweets=data_train['5'][0:50000]\n",
    "for i in tqdm(range(len(GivenTweets))):\n",
    "    tweet=GivenTweets[i].split(\" \")\n",
    "    for word in tweet:\n",
    "        if(re.match(\"@[a-z_0-9]\",word.lower())):\n",
    "            if(word in Username):\n",
    "                Username[word]+=1\n",
    "            else:\n",
    "                Username[word]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@mileycyrus\n",
      "@tommcfly\n",
      "@ddlovato\n"
     ]
    }
   ],
   "source": [
    "for i in Username:\n",
    "    if(Username[i]>100):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username1=np.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTmodel=DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTmodel.fit(Train,data_train['0'][0:50000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred=DTmodel.predict(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Actual=data_test['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.7348666666666667\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.7364461846051317\n",
      "-----------------\n",
      "Recall: 0.7342563829963953\n",
      "-----------------\n",
      "F1 Score: 0.7353496535489399\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.7332860953651217\n",
      "-----------------\n",
      "Recall: 0.7354810120232104\n",
      "-----------------\n",
      "F1 Score: 0.7343819136590944\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.7348657836040171\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              175930               63274\n",
      "Actual Positive               63990              176806\n"
     ]
    }
   ],
   "source": [
    "func_Eval(Y_Actual,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 73542.48it/s]\n",
      "100%|██████████| 480000/480000 [00:06<00:00, 72317.74it/s]\n"
     ]
    }
   ],
   "source": [
    "TrainUser1=np.zeros(len(GivenTweets))\n",
    "TrainUser2=np.zeros(len(GivenTweets))\n",
    "TrainUser3=np.zeros(len(GivenTweets))\n",
    "for i in tqdm(range(len(GivenTweets))):\n",
    "    tweet=GivenTweets[i].split(\" \")\n",
    "    for word in tweet:\n",
    "        if(\"@mileycyrus\",word.lower()):\n",
    "            TrainUser1[i]=1\n",
    "        if(\"@tommcfly\",word.lower()):\n",
    "            TrainUser2[i]=1\n",
    "        if(\"@ddlovato\",word.lower()):\n",
    "            TrainUser3[i]=1\n",
    "\n",
    "TestTweets=data_test['5']\n",
    "TestUser1=np.zeros(len(TestTweets))\n",
    "TestUser2=np.zeros(len(TestTweets))\n",
    "TestUser3=np.zeros(len(TestTweets))\n",
    "for i in tqdm(range(len(TestTweets))):\n",
    "    tweet=TestTweets[i].split(\" \")\n",
    "    for word in tweet:\n",
    "        if(\"@mileycyrus\",word.lower()):\n",
    "            TestUser1[i]=1\n",
    "        if(\"@tommcfly\",word.lower()):\n",
    "            TestUser2[i]=1\n",
    "        if(\"@ddlovato\",word.lower()):\n",
    "            TestUser3[i]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAdd=pd.DataFrame(TrainUser1)\n",
    "trainAdd[\"user2\"]=pd.DataFrame(TrainUser2)\n",
    "trainAdd[\"user3\"]=pd.DataFrame(TrainUser3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAdd=pd.DataFrame(TestUser1)\n",
    "testAdd[\"user2\"]=pd.DataFrame(TestUser2)\n",
    "testAdd[\"user3\"]=pd.DataFrame(TestUser3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAdd=csr_matrix(trainAdd)\n",
    "testAdd=csr_matrix(testAdd)\n",
    "Train1=hstack([Train,trainAdd])\n",
    "Test1=hstack([Test,testAdd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTmodel=DecisionTreeClassifier()\n",
    "DTmodel.fit(Train1,data_train['0'][0:50000])\n",
    "ypred1=DTmodel.predict(Test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.7366791666666667\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.7399471454339528\n",
      "-----------------\n",
      "Recall: 0.7325536969052642\n",
      "-----------------\n",
      "F1 Score: 0.73623185986235\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.7334547411117089\n",
      "-----------------\n",
      "Recall: 0.7408320931088109\n",
      "-----------------\n",
      "F1 Score: 0.737124958923825\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.7366784093930875\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              177210               61994\n",
      "Actual Positive               64400              176396\n"
     ]
    }
   ],
   "source": [
    "func_Eval(Y_Actual,ypred1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_Eval(Y_Actual,Y_pred):\n",
    "\n",
    "    TrueNegative=0\n",
    "    TruePositive=0\n",
    "    FalsePositive=0\n",
    "    FalseNegative=0\n",
    "    for i in range(len(Y_pred)):\n",
    "        #if the actual class is negative\n",
    "        if(Y_Actual[i]==0):\n",
    "            \n",
    "            #if the predicited class is negative\n",
    "            if(Y_pred[i]==0):\n",
    "                TrueNegative+=1\n",
    "                \n",
    "            #if the predicted class is positive\n",
    "            else:\n",
    "                FalsePositive+=1\n",
    "                \n",
    "        #if the actual class is positive\n",
    "        else:\n",
    "            #if the predicited class is positive\n",
    "            if(Y_pred[i]==4):\n",
    "                TruePositive+=1\n",
    "                \n",
    "            #if the predicited class is negative\n",
    "            else:\n",
    "                FalseNegative+=1\n",
    "\n",
    "    Confusion_Matrix=[[TrueNegative,FalsePositive],[FalseNegative,TruePositive]]\n",
    "    \n",
    "    Confusion_Matrix=pd.DataFrame(Confusion_Matrix,columns=['Predicted Negative','Predicted Positive'])\n",
    "    \n",
    "    Confusion_Matrix.rename(index={0: \"Actual Negative\", 1: \"Actual Positive\"},inplace=True)\n",
    "    \n",
    "    MyPrecision=TruePositive/(TruePositive+FalsePositive)\n",
    "\n",
    "    MyRecall=TruePositive/(TruePositive+FalseNegative)\n",
    "    \n",
    "    MyAccuracy=(TruePositive+TrueNegative)/(TruePositive+TrueNegative+FalseNegative+FalsePositive)\n",
    "    \n",
    "    MyF1score= 2*(MyPrecision*MyRecall)/(MyPrecision+MyRecall)\n",
    "    \n",
    "    MyPrecisionZero=TrueNegative/(TrueNegative+FalseNegative)\n",
    "\n",
    "    MyRecallZero=TrueNegative/(TrueNegative+FalsePositive)\n",
    "    \n",
    "    \n",
    "    MyF1scoreZero= 2*(MyPrecisionZero*MyRecallZero)/(MyPrecisionZero+MyRecallZero)\n",
    "\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Accuracy:\",MyAccuracy)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Positive CLass, Label:4\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecision)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecall)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1score)\n",
    "\n",
    "    print()\n",
    "\n",
    "    print(\"For the Negative CLass, Label:0\")\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Precision:\",MyPrecisionZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"Recall:\",MyRecallZero)\n",
    "\n",
    "    print(\"-----------------\")\n",
    "    print(\"F1 Score:\",MyF1scoreZero)\n",
    "\n",
    "    print()\n",
    "    print(\"--------\")\n",
    "    print(\"Macro Average F1\",(MyF1score+MyF1scoreZero)/2)\n",
    "    print(\"-----------------\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(Confusion_Matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('sentiment_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Actual=data_test['0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.7461958333333333\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.7430617982119216\n",
      "-----------------\n",
      "Recall: 0.755207727703118\n",
      "-----------------\n",
      "F1 Score: 0.7490855316274242\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.7494559396092967\n",
      "-----------------\n",
      "Recall: 0.7371239611377736\n",
      "-----------------\n",
      "F1 Score: 0.7432388001820971\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.7461621659047606\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              176323               62881\n",
      "Actual Positive               58945              181851\n"
     ]
    }
   ],
   "source": [
    "file=open(\"./Predictions_Ques1/ypredNB\",'rb')\n",
    "y_pred=pickle.load(file)\n",
    "func_Eval(Y_Actual,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.7352\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.7367301045267147\n",
      "-----------------\n",
      "Recall: 0.734692436751441\n",
      "-----------------\n",
      "F1 Score: 0.7357098597289397\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.7336682369616876\n",
      "-----------------\n",
      "Recall: 0.7357109412886073\n",
      "-----------------\n",
      "F1 Score: 0.7346881692598638\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.7351990144944017\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              175985               63219\n",
      "Actual Positive               63885              176911\n"
     ]
    }
   ],
   "source": [
    "file=open(\"./Predictions_Ques1/y_DT_50k\",'rb')\n",
    "y_pred=pickle.load(file)\n",
    "func_Eval(Y_Actual,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.7629208333333334\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.7784530919742855\n",
      "-----------------\n",
      "Recall: 0.7372215485307065\n",
      "-----------------\n",
      "F1 Score: 0.7572765006249493\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.7488629057223823\n",
      "-----------------\n",
      "Recall: 0.788791157338506\n",
      "-----------------\n",
      "F1 Score: 0.7683086232241094\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.7627925619245294\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              188682               50522\n",
      "Actual Positive               63276              177520\n"
     ]
    }
   ],
   "source": [
    "file=open(\"./Predictions_Ques1/yMLP50ktrain\",'rb')\n",
    "y_pred=pickle.load(file)\n",
    "func_Eval(Y_Actual,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classifier Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "Accuracy: 0.7994291666666666\n",
      "-----------------\n",
      "\n",
      "For the Positive CLass, Label:4\n",
      "-----------------\n",
      "Precision: 0.7966087085016255\n",
      "-----------------\n",
      "Recall: 0.8059643847904451\n",
      "-----------------\n",
      "F1 Score: 0.8012592378514513\n",
      "\n",
      "For the Negative CLass, Label:0\n",
      "-----------------\n",
      "Precision: 0.8023361085727824\n",
      "-----------------\n",
      "Recall: 0.7928504540057859\n",
      "-----------------\n",
      "F1 Score: 0.797565078430548\n",
      "\n",
      "--------\n",
      "Macro Average F1 0.7994121581409996\n",
      "-----------------\n",
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative              189653               49551\n",
      "Actual Positive               46723              194073\n"
     ]
    }
   ],
   "source": [
    "file=open(\"./Predictions_Ques1/ySCV50ktrain\",'rb')\n",
    "y_pred=pickle.load(file)\n",
    "func_Eval(Y_Actual,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# A library which cleans URL, Mentions, Hashtags for preprocessing tweets\n",
    "import preprocessor as p\n",
    "import re\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823\n",
      "714\n",
      "1537\n"
     ]
    }
   ],
   "source": [
    "# Reading DataSet\n",
    "cols = [\"id\", \"tweet\", \"emotion\", \"intensity\"]\n",
    "joy_data = pd.read_csv(\"joy-ratings-0to1.train.txt\", sep='\\t', names=cols, header=None)\n",
    "joy_data_test = pd.read_csv(\"joy-ratings-0to1.test.gold.txt\", sep='\\t', names=cols, header=None)\n",
    "merged=pd.concat([joy_data,joy_data_test],ignore_index=True)\n",
    "# merged=anger_data.append(anger_data_test)\n",
    "# merged\n",
    "print(len(joy_data))\n",
    "print(len(joy_data_test))\n",
    "print(len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the purpose of training and features extraction train set contains both test and train tweets\n",
    "tweets_train=merged['tweet']\n",
    "tweets_train2=merged['tweet']\n",
    "tweets_test=joy_data_test['tweet']\n",
    "tweets_test2=joy_data_test['tweet']\n",
    "# tweets_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some preprocessing here\n",
    "# Lower case words\n",
    "tweets_train=tweets_train.apply(lambda s:s.lower())\n",
    "tweets_train2=tweets_train2.apply(lambda s:s.lower())\n",
    "tweets_test=tweets_test.apply(lambda s:s.lower())\n",
    "tweets_test2=tweets_test2.apply(lambda s:s.lower())\n",
    "for i in range(len(tweets_train)):\n",
    "    #Removing Urls, mentions, hashtags\n",
    "    res=p.clean(tweets_train[i])\n",
    "    # removing punctuations\n",
    "    res = re.sub(r'[^\\w\\s]', '',res)\n",
    "    tweets_train[i]=res\n",
    "for i in range(len(tweets_test)):\n",
    "    res=p.clean(tweets_test[i])\n",
    "    # removing punctuations\n",
    "    res = re.sub(r'[^\\w\\s]', '',res)\n",
    "    tweets_test[i]=res\n",
    "# tweets_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER feature - give it tweets as it is without preprocessed\n",
    "obj= SentimentIntensityAnalyzer()\n",
    "Pos_vader=np.zeros(len(tweets_train2))\n",
    "Neg_vader=np.zeros(len(tweets_train2))\n",
    "Neu_vader=np.zeros(len(tweets_train2))\n",
    "Comp_vader=np.zeros(len(tweets_train2))\n",
    "for i in range(len(tweets_train2)):\n",
    "    sentiment_dict = obj.polarity_scores(tweets_train2[i])\n",
    "    Pos_vader[i]=sentiment_dict['pos']\n",
    "    Neg_vader[i]=sentiment_dict['neg']\n",
    "    Neu_vader[i]=sentiment_dict['neu']\n",
    "    Comp_vader[i]=sentiment_dict['compound']\n",
    "# Adding features into dataframe\n",
    "merged['Pos_vader']=Pos_vader\n",
    "merged['Neg_vader']=Neg_vader\n",
    "merged['Neu_vader']=Neu_vader\n",
    "merged['Comp_vader']=Comp_vader\n",
    "cols=cols+['Pos_vader','Neg_vader','Neu_vader','Comp_vader']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polar word count feature- using MPQA and Bing liu\n",
    "mpqa_pos=set()\n",
    "mpqa_neg=set()\n",
    "bing_pos=set()\n",
    "bing_neg=set()\n",
    "bing_data = pd.read_csv(\"./lexicons/1. BingLiu.csv\", sep='\\t', names=[\"words\",\"sentiment\"], header=None)\n",
    "for i in range(len(bing_data['words'])):\n",
    "    if bing_data['sentiment'][i]=='positive':\n",
    "        bing_pos.add(bing_data['words'][i])\n",
    "    elif bing_data['sentiment'][i]=='negative':\n",
    "        bing_neg.add(bing_data['words'][i])\n",
    "mpqa_data = pd.read_csv(\"./lexicons/2. mpqa.txt\", sep='\\t', names=[\"words\",\"sentiment\"], header=None)\n",
    "for i in range(len(mpqa_data['words'])):\n",
    "    if mpqa_data['sentiment'][i]=='positive':\n",
    "        mpqa_pos.add(mpqa_data['words'][i])\n",
    "    elif mpqa_data['sentiment'][i]=='negative':\n",
    "        mpqa_neg.add(mpqa_data['words'][i])\n",
    "polar_pos=mpqa_pos.union(bing_pos)\n",
    "polar_neg=mpqa_neg.union(bing_neg)\n",
    "polarcount_pos=np.zeros(len(tweets_train))\n",
    "polarcount_neg=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train)):\n",
    "    tweet=tweets_train[i]\n",
    "    for word in word_tokenize(tweet):\n",
    "        if word in polar_pos:\n",
    "            polarcount_pos[i]+=1\n",
    "        elif word in polar_neg:\n",
    "            polarcount_neg[i]+=1\n",
    "# Vectors polarcount_pos, polarcount_neg contains final counts\n",
    "# Adding features into dataframe\n",
    "merged['polarcount_pos']=polarcount_pos\n",
    "merged['polarcount_neg']=polarcount_neg\n",
    "cols=cols+['polarcount_pos','polarcount_neg']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate polarity scores:from lexicons: Sentiment140 (d1), Sentiwordnet(d2)\n",
    "# For D1=Sentiment140\n",
    "d1 = pd.read_csv(\"./lexicons/3. Sentiment140-Lexicon-v0.1/unigrams-pmilexicon.txt\", sep='\\t', names=['term','score','numPos','numNeg'], header=None)\n",
    "dic_d1={}\n",
    "for i in range(len(d1['term'])):\n",
    "    dic_d1[d1['term'][i]]=d1['score'][i]\n",
    "aggScore1=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train)):\n",
    "    tweet=tweets_train[i]\n",
    "    for word in word_tokenize(tweet):\n",
    "        if word in dic_d1:\n",
    "            aggScore1[i]+=dic_d1[word]\n",
    "# Vector aggScore1 contains Aggregate polarity scores for Sentiment140 data set\n",
    "# Adding features into dataframe\n",
    "merged['aggScore1']=aggScore1\n",
    "cols=cols+['aggScore1']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important libraries and functions for SentiwordNet \n",
    "# Here words are lemmatized before fetching sentiment from corpus\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "def get_sentiment(word,tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "        return []\n",
    "    lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "    if not lemma:\n",
    "        return []\n",
    "    synsets = wn.synsets(word, pos=wn_tag)\n",
    "    if not synsets:\n",
    "        return []\n",
    "    synset = synsets[0]\n",
    "    swn_synset = swn.senti_synset(synset.name())\n",
    "    return swn_synset.pos_score()-swn_synset.neg_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For D2=Sentiwordnet\n",
    "# Long code\n",
    "aggScore2=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train)):\n",
    "    tweet=word_tokenize(tweets_train[i])\n",
    "    pos_val = nltk.pos_tag(tweet)\n",
    "    senti_val = [get_sentiment(x,y) for (x,y) in pos_val]\n",
    "    for j in senti_val:\n",
    "        if type(j) is not list:\n",
    "            aggScore2[i]+=j\n",
    "# Vector aggScore2 contains Aggregate polarity scores for Sentiment140 data set\n",
    "# Adding features into dataframe\n",
    "merged['aggScore2']=aggScore2\n",
    "cols=cols+['aggScore2']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate polarity scores:from lexicons: AFFIN\n",
    "from afinn import Afinn\n",
    "afinn = Afinn(emoticons=True)\n",
    "aggScore3=np.zeros(len(tweets_train2))\n",
    "for i in range(len(tweets_train2)):\n",
    "    tweet=tweets_train2[i]\n",
    "    aggScore3[i]=afinn.score(tweet)\n",
    "# Vector aggScore3 contains Aggregate polarity scores for AFFIN data set\n",
    "# Adding features into dataframe\n",
    "merged['aggScore3']=aggScore3\n",
    "cols=cols+['aggScore3']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate polarity scores (Hashtags):NRC Hashtag Sentiment\n",
    "d1 = pd.read_csv(\"./lexicons/7. NRC-Hashtag-Sentiment-Lexicon-v0.1/unigrams-pmilexicon.txt\", sep='\\t', names=['term','score','numPos','numNeg'], header=None)\n",
    "dic_d1={}\n",
    "for i in range(len(d1['term'])):\n",
    "    dic_d1[d1['term'][i]]=d1['score'][i]\n",
    "# print(dic_d1)\n",
    "aggScoreHashtags=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train2)):\n",
    "    tweet=tweets_train2[i]\n",
    "    hashtags = [i  for i in tweet.split() if i.startswith(\"#\") ]\n",
    "    for word in hashtags:\n",
    "        if word in dic_d1:\n",
    "            aggScoreHashtags[i]+=dic_d1[word]\n",
    "        elif word[1:] in dic_d1:\n",
    "            aggScoreHashtags[i]+=dic_d1[word[1:]]\n",
    "# print(aggScoreHashtags)\n",
    "# Vector aggScoreHashtag contains (Hashtags):NRC Hashtag Sentiment\n",
    "# Adding features into dataframe\n",
    "merged['aggScoreHashtags']=aggScoreHashtags\n",
    "cols=cols+['aggScoreHashtags']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate emotion score: NRC-10 Expanded lexicon\n",
    "emotion=\"joy\"\n",
    "d_emotion=pd.read_csv(\"./lexicons/6. NRC-10-expanded.csv\", sep='\\t')\n",
    "dic_dEmo={}\n",
    "for i in range(len(d_emotion['word'])):\n",
    "    dic_dEmo[d_emotion['word'][i]]=d_emotion[emotion][i]\n",
    "aggScoreEmo=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train)):\n",
    "    tweet=tweets_train[i]\n",
    "    for word in word_tokenize(tweet):\n",
    "        if word in dic_dEmo:\n",
    "            aggScoreEmo[i]+=dic_dEmo[word]\n",
    "# print(aggScoreEmo)\n",
    "# Vector aggScoreEmo contains Aggregate emotion score\n",
    "# Adding features into dataframe\n",
    "merged['aggScoreEmo']=aggScoreEmo\n",
    "cols=cols+['aggScoreEmo']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate emotion score (Hashtags):NRC Hashtag Emotion Association Lexicon\n",
    "emotion=\"joy\"\n",
    "d_emotion=pd.read_csv(\"./lexicons/5. NRC-Hashtag-Emotion-Lexicon-v0.2.txt\", sep='\\t', names=['emotion','word','score'], header=None)\n",
    "d_emotion\n",
    "dic_dEmo={}\n",
    "for i in range(len(d_emotion['word'])):\n",
    "    if d_emotion['emotion'][i]==emotion:\n",
    "        dic_dEmo[d_emotion['word'][i]]=d_emotion['score'][i]\n",
    "aggEmoHashtags=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train2)):\n",
    "    tweet=tweets_train2[i]\n",
    "    hashtags = [j  for j in tweet.split() if j.startswith(\"#\") ]\n",
    "    for word in hashtags:\n",
    "        if word in dic_dEmo:\n",
    "            aggEmoHashtags[i]+=dic_dEmo[word]\n",
    "        elif word[1:] in dic_dEmo:\n",
    "            aggEmoHashtags[i]+=dic_dEmo[word[1:]]\n",
    "# print(aggEmoHashtags)\n",
    "# Vector aggEmoHashtags conatains Aggregate emotion score (Hashtags)\n",
    "# Adding features into dataframe\n",
    "merged['aggEmoHashtags']=aggEmoHashtags\n",
    "cols=cols+['aggEmoHashtags']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoticons score: Positive and negative AFINN project (Nielsen, 2011)\n",
    "\n",
    "emoticons=pd.read_csv(\"./lexicons/9. AFINN-emoticon-8.txt\", sep='\\t', names=['emoticon','score'], header=None)\n",
    "dic_emoji={}\n",
    "for i in (range(len(emoticons['emoticon']))):\n",
    "    dic_emoji[emoticons['emoticon'][i]]=emoticons['score'][i]\n",
    "# print(dic_emoji)\n",
    "emoji = re.compile('[\\\\u203C-\\\\u3299\\\\U0001F000-\\\\U0001F644]')\n",
    "emojiScore=np.zeros(len(tweets_train2))\n",
    "for i in (range(len(tweets_train2))):\n",
    "    tweet=(tweets_train2[i].split(\" \"))\n",
    "    for j in range(len(tweet)):\n",
    "        if(tweet[j] in dic_emoji):\n",
    "#             print(i,dic_emoji[i])\n",
    "            emojiScore[i]+=dic_emoji[tweet[j]]\n",
    "len(emojiScore)\n",
    "# for i in emojiScore:\n",
    "#     print(i,end=' ')\n",
    "# Vector emojiScore contains the required feature\n",
    "# Adding features into dataframe\n",
    "merged['emojiScore']=emojiScore\n",
    "cols=cols+['emojiScore']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of Negating words\n",
    "neg_count=np.zeros(len(tweets_train))\n",
    "for i in range(len(tweets_train)):\n",
    "    tweet=tweets_train[i]\n",
    "    res = re.findall(r'(?:^|\\W)?(never|no|nothing|nowhere|noone|none|not|havent|hasnt|hadnt|cant|couldnt|shouldnt|wont|wouldnt|dont|doesnt|didnt|isnt|arent|aint)(?:$|\\W)+',tweet)\n",
    "    neg_count[i]=len(res)\n",
    "# print(neg_count)\n",
    "# Vector neg_count contains the count of negating words\n",
    "# Adding features into dataframe\n",
    "merged['neg_count']=neg_count\n",
    "cols=cols+['neg_count']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion word count: Count of the number of words matching each emotion from NRC Word-Emotion Association Lexicon\n",
    "emotion=\"joy\"\n",
    "emotion_count=np.zeros(len(tweets_train))\n",
    "emotions_df=pd.read_csv(\"./lexicons/8. NRC-word-emotion-lexicon.txt\", sep='\\t', names=['word','emotion','score'], header=None)\n",
    "# This set contains words with Emotion X=1 in above dataframe\n",
    "set_emo=set()\n",
    "for i in range(len(emotions_df['word'])):\n",
    "    if emotions_df['emotion'][i]==emotion and emotions_df['score'][i]==1:\n",
    "        set_emo.add(emotions_df['word'][i])\n",
    "for i in range(len(tweets_train)):\n",
    "    tweet=word_tokenize(tweets_train[i])\n",
    "    for word in tweet:\n",
    "        if word in set_emo:\n",
    "            emotion_count[i]+=1\n",
    "# Vector emotion_count contains the required feature vector\n",
    "# Adding features into dataframe\n",
    "merged['emotion_count']=emotion_count\n",
    "cols=cols+['emotion_count']\n",
    "# merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N grams features ectraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# ngram_range parameter (1,2) means that unigram and bigram will be taken\n",
    "# Count Vecotrizer automatically preprocess the tweets\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2),min_df =5)\n",
    "TempVector=vectorizer.fit(tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(820, 1537)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N grams continued\n",
    "Vocab_ngrams=TempVector.get_feature_names()\n",
    "print(len(Vocab_ngrams))\n",
    "vectorizer2 = CountVectorizer(ngram_range=(1,2), vocabulary=Vocab_ngrams)\n",
    "ngrams=vectorizer2.fit_transform(tweets_train)\n",
    "x=np.transpose(ngrams.toarray())\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding N grams to dataframe\n",
    "vocab=vectorizer2.get_feature_names()\n",
    "for i in range(len(vocab)):\n",
    "    merged['gram-'+vocab[i]]=x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus features\n",
    "# identifying sentiment in tweets expressed by slang words using Slang Sentiment Dictionary (SlangSD)\n",
    "slangs= pd.read_csv(\"./lexicons/SlangSD/SlangSD.txt\", sep='\\t', names=['word','score'], header=None)\n",
    "slangScore=np.zeros(len(tweets_train2))\n",
    "dic_slangs={}\n",
    "for i in (range(len(slangs['word']))):\n",
    "    dic_slangs[slangs['word'][i]]=slangs['score'][i]\n",
    "# print(dic_slangs)\n",
    "for i in (range(len(tweets_train2))):\n",
    "    tweet=word_tokenize(tweets_train2[i])\n",
    "    for j in range(len(tweet)):\n",
    "        if(tweet[j] in dic_slangs):\n",
    "# #             print(i,dic_emoji[i])\n",
    "            slangScore[i]+=dic_slangs[tweet[j]]\n",
    "# for i in slangScore:\n",
    "#     print(i,end='  ')\n",
    "# print(tweets_train2)\n",
    "\n",
    "# Count of capitilized words\n",
    "capCount=np.zeros(len(tweets_train2))\n",
    "original_tweets=merged['tweet']\n",
    "for i in (range(len(original_tweets))):\n",
    "    tweet=word_tokenize(original_tweets[i])\n",
    "    for j in range(len(tweet)):\n",
    "        if(tweet[j].isupper()):\n",
    "            capCount[i]+=1\n",
    "# for i in capCount:\n",
    "#     print(i,end='  ')\n",
    "\n",
    "# Adding features into dataframe\n",
    "merged['slangScore']=slangScore\n",
    "# merged['capCount']=capCount\n",
    "cols=cols+['slangScore','capCount']\n",
    "# merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spltting features and y columns\n",
    "y=merged['intensity']\n",
    "x=merged.iloc[:,4:]\n",
    "df_out=merged[[\"id\", \"tweet\", \"emotion\", \"intensity\"]]\n",
    "# x=merged[x.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating train and test set before training\n",
    "y_train=y.iloc[:823]\n",
    "y_test=y.iloc[823:]\n",
    "x_train=x.iloc[:823]\n",
    "x_test=x.iloc[823:]\n",
    "df_out=df_out[823:]\n",
    "# print(len(y_train))\n",
    "# print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for DT= 0.05419594072082166\n",
      "MAE for DT= 0.1839493557422969\n"
     ]
    }
   ],
   "source": [
    "# Running Decison Tree regressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor   \n",
    "# create a regressor object \n",
    "regressor = DecisionTreeRegressor(random_state = 0)  \n",
    "# fit the regressor with X and Y data \n",
    "regressor.fit(x_train, y_train) \n",
    "y_pred=regressor.predict(x_test)\n",
    "print(\"MSE for DT=\",mean_squared_error(y_test, y_pred))\n",
    "print(\"MAE for DT=\",mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output file for model results\n",
    "df_out['intensity']=y_pred\n",
    "df_out.to_csv('dt_joy.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for SVM= 0.028213143180404944\n",
      "MAE for SVM= 0.13397292644048686\n"
     ]
    }
   ],
   "source": [
    "# Runing SVM\n",
    "from sklearn.svm import SVR\n",
    "SVRregr=SVR()\n",
    "SVRregr.fit(x_train,y_train)\n",
    "y_pred=SVRregr.predict(x_test)\n",
    "print(\"MSE for SVM=\",mean_squared_error(y_test, y_pred))\n",
    "print(\"MAE for SVM=\",mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output file for model results\n",
    "df_out['intensity']=y_pred\n",
    "df_out.to_csv('SVM_joy.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for MLP= 0.0406824680358146\n",
      "MAE for MLP= 0.16176728440578847\n"
     ]
    }
   ],
   "source": [
    "#Running MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "MLP = MLPRegressor()\n",
    "MLP.fit(x_train, y_train)\n",
    "y_pred=MLP.predict(x_test)\n",
    "print(\"MSE for MLP=\",mean_squared_error(y_test, y_pred))\n",
    "print(\"MAE for MLP=\",mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output file for model results\n",
    "df_out['intensity']=y_pred\n",
    "df_out.to_csv('MLP_joy.txt',sep='\\t',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating output file for all the features\n",
    "merged.to_csv('Features_joy.txt',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
